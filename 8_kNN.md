## Метод к-го ближайшего соседа (K-Nearest Neighbors, kNN)

Когда-то был основным инструментом при создании рекомендательных сервисов. Сейчас метод вытеснен конкурентами, почти не применяется.
Но еще не вечер. Обладает прекрасными математическими свойствами.

### Метод k-го ближайшего соседа

$k=1$ (один сосед), тогда объект относится к тому же классу, что и его ближайший сосед.

$k>1$, тогда объект относится к тому классу, члены которого составляют большинство среди $k$ ближайших соседей.


#### Технические детали

Если невозможно найти ровно $k$ соседей, значение $k$ увеличивают (вариант: выбрать случайно).

Если невозможно определить, какой класс составляет большинство среди $k$ соседей, значение $k$  увеличивают(вариант: выбрать случайно).

Если классов два,  удобно использовать нечетные значения $k$.

#### Выбор k: общие соображения

В теории: правильно увеличивать $k$ с ростом объема обучающей выборки $n$.На практике: находим $k$ кросс-валидацией
Из общих соображений ясно, что  $k$ должно быть разным в зависимости от локальной плотности точек, но пока неизвестно каким... В разреженном месте уменьшаем чисо $k$ (расстояния будут большие, значит они менее похожи), а в плотном увеличиваем (больше информации, уменьшаем доверительный интервал).

1. 1-я модификация метода  knn

	+ Модифицированный метод  Knn оценивает вероятность принадлежать классу
	+ Нужна калибровка

2. 2-я модификация метода knn

	+ Модифицированный метод Knn оценивает вероятность принадлежать классу
	+ вес соседа обратно пропорционален расстоянию до него (Предложенная схема не идеальна. Например, можно веса вычислять с использованием ядер. Как в ядерных оценках плотности (самостоятельно). Надо предотвратить деление на ноль (самостоятельно))
	+ Нужна калибровка

#### Параметры

Аналитик подбирает всего два параметра: $k$ - число ближайших соседей; способ вычисления расстояния между объектами (расстояние измеряет сходство объектов).

### Метод k-го ближайшего соседа состоятельный

+ Байесовский классификатор имеет максимальную точность (accuracy)
+ Но он требует знания совместного распределения предикторов и отклика.
+ Метод состоятельный, если его точность стремится к точности байесовского классификатора, когда число наблюдений стремится к бесконечности.

**Достоинство**: Свободный от распределения.

+ Нет предположений о распределении данных (при доказательстве состоятельности предполагаем существование плотности).
+ При этом метод состоятельный!

#### Новые наблюдения

Легко обновлять; просто добавляем новые наблюдения в обучающую выборку

**Требуется Стандартизация переменных! Потому что используются расстояния**

### Недостатки алгоритма 

+ Деградирует, если в данных шум и малоинформативные переменные. Лекарство: Отбор переменных и feature engineering
+ Требует большого объема памяти. Лекарство: экономно храним обучающую выборку, в Питоне в виде  R-tree или kd-tree.
+ Плохо работает, когда не сбалансированы объемы классов: доминирующий класс составляет большинство среди соседей. Лекарство: выравнивание количества наблюдений в разных классах на этапе обучения. От несбалансированности страдают почти все модели

**lazy learning**. Нет модели, но храним в памяти всю обучающую выборку.

#### Возрождение kNN

KNNImputer — заполнение пропусков

### KNN в задачах регрессии

Находим $k$ ближайших соседей точки $X$.

$Y = f(x)$ определяем как (взвешенное) среднее откликов ($Y$-ков) у $k$ ближайших соседей.

### Данные: попарные расстояния

Метод KNN работает даже если неизвестны координаты точек, но можем сосчитать расстояния между любыми двумя точками

